{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle as pkl\n",
    "from joblib import load, dump\n",
    "import cv2\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from tqdm import tqdm\n",
    "from tensorflow.python.client import device_lib\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL_LIST = []\n",
    "MODEL_NAMES = ['skin_disease_resnet_No_AUG_88acc', 'resnet_model1_cifar10']\n",
    "\n",
    "for i in range(len(MODEL_NAMES)):\n",
    "    ### Loading DNN models \n",
    "    with open('Models/'+ MODEL_NAMES[i]+'.json', 'r') as j_file:\n",
    "        loaded_model_json = j_file.read()\n",
    "    model = tf.keras.models.model_from_json(loaded_model_json)\n",
    "    model.load_weights(\"Models/\"+MODEL_NAMES[i]+\".h5\")\n",
    "    if (MODEL_NAMES[i] == 'skin_disease_resnet_No_AUG_88acc'):\n",
    "        model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=2e-5), loss='binary_crossentropy',\n",
    "                      metrics=['acc'])\n",
    "    else:\n",
    "        model.compile(optimizer = tf.keras.optimizers.Adam(0.001),\n",
    "                      loss = \"categorical_crossentropy\",  metrics= ['accuracy'])\n",
    "    MODEL_LIST.append(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cifar 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cifar model benchmark\n",
    "def loadPreprocessCifar10():\n",
    "    ### Load In Data\n",
    "    (trainX, trainY), (testX, testY) = tf.keras.datasets.cifar10.load_data()\n",
    "    trainY = tf.keras.utils.to_categorical(trainY)\n",
    "    testY = tf.keras.utils.to_categorical(testY)\n",
    "    return [trainX/255, trainY, testX/255, testY]\n",
    "\n",
    "trainX, trainY, testX, testY = loadPreprocessCifar10()\n",
    "extractor0  = tf.keras.Model(inputs = MODEL_LIST[0].input,\n",
    "                         outputs = MODEL_LIST[0].get_layer(MODEL_LIST[0].layers[-5].name).output\n",
    "                        )\n",
    "extractor1  = tf.keras.Model(inputs = MODEL_LIST[1].input,\n",
    "                         outputs = MODEL_LIST[1].get_layer(MODEL_LIST[1].layers[-5].name).output\n",
    "                        )\n",
    "### Predictions\n",
    "#p = model1.predict(trainX,verbose = 1)\n",
    "print(\"Model Evaluation\")\n",
    "MODEL_LIST[1].evaluate(testX, testY, verbose =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting Features......\")\n",
    "print(\"May take a while\")\n",
    "feat0 = []\n",
    "feat1 = []\n",
    "for i in tqdm(range(len(trainX))):\n",
    "    tmp = np.expand_dims(cv2.resize(trainX[i], (200,200)), axis = 0)\n",
    "    feat0.append(extractor0.predict(tmp))\n",
    "    feat1.append(extractor1.predict(np.expand_dims(trainX[i], axis = 0)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save(\"data/train_feat0.npy\", np.vstack(feat0))\n",
    "#np.save(\"data/train_feat1.npy\", np.vstack(feat1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting Features......\")\n",
    "print(\"May take a while\")\n",
    "test_feat0 = []\n",
    "test_feat1 = []\n",
    "for i in tqdm(range(len(testX))):\n",
    "    tmp = np.expand_dims(cv2.resize(testX[i], (200,200)), axis = 0)\n",
    "    test_feat0.append(extractor0.predict(tmp))\n",
    "    test_feat1.append(extractor1.predict(np.expand_dims(testX[i], axis = 0)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"data/test_feat0.npy\", np.vstack(test_feat0))\n",
    "np.save(\"data/test_feat1.npy\", np.vstack(test_feat1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc1 = SVC()\n",
    "svc1.fit(np.vstack(feat1),np.argmax(trainY, axis = 1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score \n",
    "pred = svc1.predict(np.vstack(feat1))\n",
    "acc = accuracy_score(pred, np.argmax(trainY, axis = 1) )\n",
    "print(\"The accuracy of the support vector Machine: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Performing predictions....\")\n",
    "print(\"May take a while\")\n",
    "pred0 = []\n",
    "pred1 = []\n",
    "for i in tqdm(range(len(testX))):\n",
    "    tmp = np.expand_dims(cv2.resize(testX[i], (200,200)), axis = 0)\n",
    "    pred0.append(MODEL_LIST[0].predict(tmp))\n",
    "    pred1.append(MODEL_LIST[1].predict(np.expand_dims(testX[i], axis = 0)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"data/test_feat0.npy\", np.vstack(feat0))\n",
    "np.save(\"data/test_feat1.npy\", np.vstack())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "correct_model_predictions_idx = []\n",
    "for i in range(len(pred0)):\n",
    "    if(np.amax(pred0[i])>np.amax(pred1[i])):\n",
    "        n = n+1\n",
    "    else:\n",
    "        correct_model_predictions_idx.append(i)\n",
    "print(\"{} samples were passed to the wrong model.\".format(n))\n",
    "print(\"This is because the wrong model had a higher confidence value.\")\n",
    "pidx = np.argmax(np.vstack(pred1), axis = 1)[correct_model_predictions_idx]\n",
    "groundTruth = np.argmax(testY[correct_model_predictions_idx],axis = 1)\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(groundTruth,pidx)\n",
    "print(\"Accuracy\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, roc_curve\n",
    "f1 = f1_score(groundTruth, pidx, average = \"macro\")\n",
    "acc = accuracy_score(groundTruth,pidx)\n",
    "recall = recall_score(groundTruth, pidx, average = \"macro\")\n",
    "precision =precision_score(groundTruth, pidx, average = \"macro\")\n",
    "print(\"Accuracy: {}\".format(acc))\n",
    "print(\"F1 Score: {}\".format(f1))\n",
    "print(\"recall Score: {}\".format(recall))\n",
    "print(\"Precision Score: {}\".format(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "conf = tf.math.confusion_matrix(groundTruth,pidx)\n",
    "font = {'family' : 'sans',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 10}\n",
    "matplotlib.rc('font', **font)\n",
    "labels_names = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship',\n",
    "                'truck']\n",
    "df_cm = pd.DataFrame(np.array(conf), index = [i for i in labels_names],\n",
    "                  columns = [i for i in labels_names])\n",
    "plt.figure(figsize = (20,10))\n",
    "sn.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_auc_score\n",
    "y_test = np.copy(testY[correct_model_predictions_idx])\n",
    "y_score = np.copy(np.vstack(pred1)[correct_model_predictions_idx])\n",
    "n_classes =  8\n",
    "lw =2\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure(figsize = (20, 30))\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FADL on The Skin Disease Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnd_truth = pd.read_csv(\"data\\SkinDiseaseDataset\\ISIC_2019_Training_GroundTruth.csv\")\n",
    "#aug_df = pd.read_csv(\"data/augmentImageNameLabels.csv\")\n",
    "gnd_truth.head()\n",
    "\n",
    "IMG_NAMES = gnd_truth['image']\n",
    "TRAIN_IMG_RT_PATH = 'data/ISIC_2019_Training_Input/'\n",
    "TEST_IMG_RT_PATH = 'data/ISIC_2019_Test_Input'\n",
    "### No Examples of UNK so drop it\n",
    "labels = np.array(gnd_truth.drop(columns = [\"image\", \"UNK\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c, freq = np.unique(np.argmax(labels, axis= 1), return_counts = True)\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"# of Occurences\")\n",
    "plt.bar(c,freq)\n",
    "print(\"# of occurences\")\n",
    "print(freq)\n",
    "print(\"Classes\")\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnClassIndex(trainY,class_index):\n",
    "    labels = np.argmax(trainY, axis = 1)\n",
    "    return [ i for i in range(len(labels)) if labels[i] == class_index]\n",
    "\n",
    "\n",
    "### Randomly Balance Sample from classes  \n",
    "smpl_test_idxs = []\n",
    "smpl_train_idxs = []\n",
    "for i in range(len(labels[1])):\n",
    "    idxs = np.array(returnClassIndex(labels, i))\n",
    "    sel = np.random.RandomState(seed = 42).permutation(len(idxs))\n",
    "    #print(len(idxs))\n",
    "    ### Shuffle\n",
    "    train_sel = list(sel[:int(0.75*len(idxs))])\n",
    "    test_sel = list(sel[int(0.75*len(idxs)):])\n",
    "    smpl_train_idxs += list(idxs[train_sel])\n",
    "    smpl_test_idxs += list(idxs[test_sel])\n",
    "\n",
    "paths = np.array([TRAIN_IMG_RT_PATH + m +'.jpg' for m in IMG_NAMES ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_size =200\n",
    "channels = 3\n",
    "num_classes = 8\n",
    "lr = 0.001 ### learning rate\n",
    "input_shape = ( im_size, im_size, channels)\n",
    "l1_lambda = 0 ### L1 regularization lambda parameter\n",
    "l2_lambda = 0\n",
    "keep_prob =  0\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dataframes for Generator\n",
    "train_df = pd.DataFrame({\"filename\":paths[smpl_train_idxs],\n",
    "                        \"class\": np.argmax(labels, axis = 1)[smpl_train_idxs].astype(str)})\n",
    "test_df = pd.DataFrame({\"filename\":paths[smpl_test_idxs],\n",
    "                        \"class\": np.argmax(labels, axis = 1)[smpl_test_idxs].astype(str)})\n",
    "\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        rescale=1.0/255,\n",
    "        samplewise_center = True,\n",
    "        samplewise_std_normalization = True, \n",
    "        horizontal_flip=True,\n",
    "        vertical_flip = True,\n",
    "        shear_range=0.5,\n",
    "        zoom_range=0.5,\n",
    "        rotation_range = 30\n",
    ")\n",
    "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe = train_df,\n",
    "    class_mode = 'categorical',\n",
    "    y_col=\"class\",\n",
    "    batch_size = batch_size, \n",
    "    target_size = (im_size, im_size)\n",
    ")\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe = test_df,\n",
    "    class_mode = 'categorical',\n",
    "    y_col=\"class\",\n",
    "    batch_size = batch_size,\n",
    "    target_size = (im_size, im_size)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_LIST[0].evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Performing predictions....\")\n",
    "print(\"May take a while\")\n",
    "pred1 = []\n",
    "pred0 = []\n",
    "labels = []\n",
    "for i in tqdm(range(len(test_generator) -1)):\n",
    "    tmp = next(test_generator)\n",
    "    for i0 in range(16):\n",
    "        pred1.append(MODEL_LIST[1].predict(\n",
    "            np.expand_dims(cv2.resize(tmp[0][i0], (32, 32)), axis = 0)))\n",
    "    pred0.append(MODEL_LIST[0].predict(tmp[0]))\n",
    "    labels.append(tmp[1])\n",
    "p0 = np.vstack(pred0)\n",
    "p1 = np.vstack(pred1)\n",
    "test_labels = np.vstack(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0 = np.vstack(pred0)\n",
    "p1 = np.vstack(pred1)\n",
    "test_labels = np.vstack(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "correct_model_predictions_idx = []\n",
    "for i in tqdm(range(len(p0))):\n",
    "    if(np.amax(p1[i])>np.amax(p0[i])):\n",
    "        n = n+1\n",
    "    else:\n",
    "        correct_model_predictions_idx.append(i)\n",
    "print(\"{} samples were passed to the wrong model.\".format(n))\n",
    "print(\"This is because the wrong model had a higher confidence value.\")\n",
    "pidx = np.argmax(p0, axis = 1)[correct_model_predictions_idx]\n",
    "groundTruth = np.argmax(test_labels, axis = 1)[correct_model_predictions_idx]\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(groundTruth,pidx)\n",
    "print(\"Accuracy\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "conf = tf.math.confusion_matrix(groundTruth,pidx)\n",
    "font = {'family' : 'sans',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 10}\n",
    "matplotlib.rc('font', **font)\n",
    "labels_names = list(gnd_truth.columns[1:-1])\n",
    "df_cm = pd.DataFrame(np.array(conf), index = [i for i in labels_names],\n",
    "                  columns = [i for i in labels_names])\n",
    "plt.figure(figsize = (20,10))\n",
    "sn.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, roc_curve\n",
    "f1 = f1_score(groundTruth, pidx, average = \"macro\")\n",
    "acc = accuracy_score(groundTruth,pidx)\n",
    "recall = recall_score(groundTruth, pidx, average = \"macro\")\n",
    "precision =precision_score(groundTruth, pidx, average = \"macro\")\n",
    "print(\"Metrics\")\n",
    "print()\n",
    "print(\"Accuracy: {}\".format(acc))\n",
    "print(\"F1 Score: {}\".format(f1))\n",
    "print(\"recall Score: {}\".format(recall))\n",
    "print(\"Precision Score: {}\".format(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_auc_score\n",
    "y_test = np.copy(test_labels[correct_model_predictions_idx])\n",
    "y_score = np.copy(p0[correct_model_predictions_idx])\n",
    "n_classes =  8\n",
    "lw =2\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure(figsize = (20, 30))\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepdive",
   "language": "python",
   "name": "deepdive"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
